{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>\n",
    "Assignment 5: PCA, Naive Bayes, Clustering, and Deep Learning\n",
    "</center>\n",
    "</h1>\n",
    "<center>\n",
    "CS 4262/5262 - Foundations of Machine Learning<br>\n",
    "Vanderbilt University, Spring 2023<br>\n",
    "Due: Check Brightspace\n",
    "</center>\n",
    "<hr>\n",
    "<br>This assignment will focus on: (1) PCA, (2) Clustering, and (3) deep learning. In addition to programming tasks, there are short-answer questions throughout the notebook. \n",
    "\n",
    "Contact: Quan Liu quan.liu@vanderbilt.edu for any clarifying questions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please enter your name:  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 1: Eigencats\n",
    "\n",
    "\n",
    "In this part, you will be working with a dataset of cat images. You will apply principal component analysis (PCA) to decompose the data into \"eigencats\", and use the eigencat basis to form low-dimensional approximations of the original data.\n",
    "\n",
    "- Load the cat dataset from the file \"cats.csv\" (should be included in the git repo; here is the original source: https://github.com/bioramble/pca). This dataset consists of a 4096 x 80 matrix of \"pre-flattened\" 64 x 64 images of cats (i.e., each of eighty 64 x 64 image has been flattened into a vector of length 4096).\n",
    "- A function for loading the dataset has been provided below, so you just need to call it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for loading dataset into matrix\n",
    "# returns a matrix where each of the 80 rows represents a cat image \n",
    "# and the 4096 columns represent features (pixels) of the images\n",
    "def load_cat_file(file_path):\n",
    "    cat_matrix = np.zeros((80,4096))\n",
    "    with open(file_path) as f:\n",
    "        cat_reader = csv.reader(f, delimiter=',')\n",
    "        for i, line in enumerate(cat_reader):\n",
    "            pixels = [int(p) for p in line]\n",
    "            cat_matrix[:,i] = pixels\n",
    "    return cat_matrix\n",
    "            \n",
    "cat_matrix = load_cat_file('cat.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The next block of code provides a function for visualizing a single cat image, given a flattened image as input.\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for visualizing a cat image\n",
    "def vis_cat_image(cat_vec, axis=None):\n",
    "    cat_img = np.reshape(cat_vec, (64,64)).T \n",
    "    if axis is None:\n",
    "        plt.imshow(cat_img, interpolation='nearest')\n",
    "        plt.show()\n",
    "    else: \n",
    "        axis.imshow(cat_img, interpolation='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * To familiarize yourself with the data, use the function above to display a single cat image of your choosing.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Next, display the average over all 80 cat images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will next perform PCA on the cat dataset to extract a set of \"eigencats\".\n",
    "- Recall that it is important to perform feature scaling prior to PCA. First, please mean-center and standardize over features in the cat dataset.\n",
    "- Next, calculate the eigenvectors and eigenvalues of the covariance matrix. **Do not use PCA from scikit-learn**, but you can use numpy functions. Please be sure to read and understand the documentation of those functions. \n",
    "- Display (as images) the top 16 \"eigencats\" (i.e., the 16 eigenvectors corresponding to the largest 16 eigenvalues). You can use the function `vis_cat_image` above. Place a title on each image that indicates which of the eigencats is displayed, where 'eigencat 1' corresponds to the largest eigenvalue and 'eigencat 16' corresponds to the 16th-largest eigenvalue.\n",
    "- Display a plot of the eigenvalues, ordered from largest to smallest. Include a title and label the axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import linalg as LA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you will use your eigencats to form a low-dimensional approximation of the data. Each original (scaled) cat image can be approximated by a weighted sum of eigencats, where the weight associated with the $i^{th}$ eigencat corresponds to the projection of the original cat image onto the $i^{th}$ eigencat. Since the eigencats have norm = 1 (you can verify this!), the projection of an image onto each eigencat is simply the inner product between the image and the eigencat.\n",
    "\n",
    "- Select an image of your choice from the original (scaled) dataset. \n",
    "   * Calculate the projection of this image onto the set of all principal components. This will result in a vector of length 4096, whose entries correspond to weights associated with each eigencat.\n",
    "   * Reconstruct the image from the set of *all* principal components. Display the resulting image.\n",
    "   * Reconstruct the image using only 1,2,8,16, and 32 principal components. Display all resulting images.\n",
    "- Repeat the above steps using another randomly selected image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1** Write down some of your observations about the appearance of the eigencats.\n",
    "\n",
    "**Question 2** What happens to the reconstructed image as you include more and more principal components? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 2: Clustering\n",
    "\n",
    "\n",
    "In this part, we will perform clustering on the cat images using k-means. Since the images are quite high-dimensional (4096 dimensions), we will use PCA to project the data into a lower-dimensional space prior to clustering. \n",
    "\n",
    "**Question 3**  Based on your analysis in Part 1, how many principal components are needed to explain 90% of the variance in the data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Project each cat image onto the set of $L$ eigencats that capture 90% variance in the data. Now, each image is represented by $L$ coordinates (i.e., its projection onto $L$ eigencats) instead of 4096.\n",
    "- Perform k-means clustering on the resulting vectors, using K=4. You may use functions from scikit-learn.\n",
    "- This results in 4 cluster centroids. Project those cluster centroids back into the original (4096-dimensional) space, and display each of those 4 centroid cats using the function `viz_cat_image` above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## Part 3: Deep Learning for Sentiment Analysis\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will be using Keras/PyTorch to build a fully-connected, feed-forward neural network for sentiment analysis. The [dataset](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) you will be working with contains 50,000 movie reviews from IMDB and the corresponding labels, where the label indicates whether the review was positive or negative. \n",
    "\n",
    "If you are using Keras, make sure you install the tensorflow at first since keras is just a wrapper API package for tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb \n",
    "\n",
    "# todo: add more keras/pytorch imports as needed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To begin, run the code block below to load the training and test sets of IMDB data from Keras/PyTorch. Each review (sample) is represented as a sequence of integers, one integer for each word in that review, where the integer assigned to a given word corresponds to the index of that word in the vocabulary. Note that below, we will turn these samples into fixed-length binary vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetch dataset from keras (already has a 50/50 train/test split)\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000, seed=113)\n",
    "\n",
    "# concatenate the data (N = 50,000)\n",
    "X = np.concatenate((x_train, x_test), axis=0)\n",
    "y = np.concatenate((y_train, y_test), axis=0)\n",
    "\n",
    "# transform X such that every input is the same length \n",
    "## 1 if input x contains the word, 0 otherwise\n",
    "## so, we are ignoring word frequency \n",
    "X_vec = np.zeros((50000,10000))\n",
    "for i, x in enumerate(X):\n",
    "    X_vec[i, x] = 1\n",
    "\n",
    "# 80/20 train/test split\n",
    "x_train = X_vec[:40000]\n",
    "y_train = y[:40000]\n",
    "x_test = X_vec[40000:]\n",
    "y_test = y[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Using the Keras/PyTorch framework, build a fully-connected, feed-forward neural network for the IMDB dataset. This network should take in the binary vector representations of movie reviews and predict the positive/negative sentiment of the reviews. \n",
    "    * Take advantage of Keras’ online documentation. Additional supplemental materials by the TA may be referenced, but should not be used as a template for your homework.\n",
    "- Hint: use binary cross-entropy as your loss function. \n",
    "- **Submit a neural network that reaches >= 85% accuracy on the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4** Specify the optimization method and activation function(s) that you used.\n",
    "\n",
    "**Question 5** Provide a brief reflection on the process by which you tweaked your network to improve its performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Submission \n",
    "\n",
    "Please upload a clean version of your work to Brightspace by the deadline. Below, please acknowledge your collaborators as well as any resources/references (beyond Keras and guides to Python syntax) that you have used in this assignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
